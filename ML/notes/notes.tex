\documentclass[8pt]{article}
\usepackage[UTF8]{ctex}
\usepackage[a4paper]{geometry}

\usepackage{amsthm,amsmath,amssymb}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{tabularx}
\usepackage{color}
\usepackage{hyperref}
\usepackage{ulem}
\usepackage{multirow}
\usepackage[cache=false]{minted}
\hypersetup{
	colorlinks=True,
	linkcolor=blue
}

\usepackage{appendix}
\geometry{a4paper,centering,scale=0.8}
\geometry{left=2.0cm, right=2.0cm, top=2.5cm, bottom=2.5cm}
\usepackage[format=hang,font=small,textfont=it]{caption}
\usepackage[nottoc]{tocbibind}

\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{extarrows}
\usepackage{qcircuit}
\usepackage{fancyhdr}
\usepackage{fancyvrb}
\usepackage{bbm}

\fvset{showspaces=True}
\SaveVerb{verbspace}! !
\newcommand{\aspace}{\UseVerb{verbspace}}%
\usepackage{cleveref}

\usepackage{pgf}
\usepackage{totpages}
\usepackage{tikz}  
\usetikzlibrary{arrows,automata}

\usetikzlibrary{arrows.meta}%画箭头用的包

\makeatletter
\def\@maketitle{%
	\newpage
	\begin{center}%
		\let \footnote \thanks
		{\LARGE \@title \par}%
		\vskip 1.5em%
		{\large
			\lineskip .5em%
			\begin{tabular}[t]{c}%
				\@author
			\end{tabular}\par}%
		\vskip 1em%
		{\large \@date}%
	\end{center}%
	\par
	\vskip 1.5em}
\makeatother

\newtheoremstyle{compact}%
{3pt}{3pt}%
{}{}%
{\bfseries}{\textcolor{red}{.}}%  % Note that final punctuation is omitted.
{.5em}{\mbox{\textcolor{red}{\thmname{#1}\thmnumber{ #2}}\thmnote{ (\textcolor{blue}{#3})}}}
\theoremstyle{compact}
\newtheorem{innercustomgeneric}{\customgenericname}
\providecommand{\customgenericname}{}
\newcommand{\newcustomtheorem}[2]{%
	\newenvironment{#1}[1]
	{%
		\renewcommand\customgenericname{#2}%
		\renewcommand\theinnercustomgeneric{##1}%
		\innercustomgeneric
	}
	{\endinnercustomgeneric}
}

\DeclareMathOperator{\card}{card}

\newtheorem{theorem}{定理}[section]
\newtheorem{lemma}[theorem]{引理}
\newtheorem{definition}[theorem]{定义}
\newtheorem{proposition}[theorem]{命题}
\newtheorem{corollary}[theorem]{推论}
\newtheorem{example}[theorem]{例}
\newtheorem{claim}[theorem]{声明}
\newtheorem{remark}[theorem]{注}
\newtheorem{thesis}[theorem]{论点}
%\newtheorem{Proof}{证明}

\def\obj#1{\textbf{\uline{#1}}}
\def\num#1{\textnormal{\textbf{\mbox{\textcolor{blue}{(#1)}}}}}
\def\le{\leqslant}
\def\ge{\geqslant}
\def\im{\text{im }}
\def\P#1{\mathbb{P}\left[{#1}\right]}
\def\E#1{\mathbb{E}\left[{#1}\right]}
\def\Var#1{\text{Var}\left[{#1}\right]}
\def\rep#1{\llcorner{#1}\lrcorner}
\def\e{\mathrm{e}}




\title{\heiti\zihao{1} 机器学习\ 课程笔记}
\author{\kaishu\zihao{-3} 酥雨\\zusuyu@stu.pku.edu.cn}

\CTEXoptions[today=old]
\date{\today}

\begin{document}
\fancypagestyle{plain}{
	\fancyhf{}
	\lhead{机器学习\ 课程笔记}
	\chead{\today}
	\rhead{ML notes}
	\cfoot{第 \thepage 页, 共 \pageref{TotPages} 页}
}
\pagestyle{plain}

\crefname{theorem}{定理}{定理}
\crefname{lemma}{引理}{引理}
\crefname{proposition}{命题}{命题}
\crefname{remark}{注}{注}
\crefname{figure}{图}{图}
\crefname{table}{表}{表}	
\maketitle
\tableofcontents
\newpage

\section{Inequalities \& Concentration Bounds}
\begin{theorem}[Markov Inequality]
	如果非负随机变量 $X$ 期望存在, 则对于任意 $k > 0$,  $$\P{X \ge k} \le \frac{\E{X}}{k}$$
	进一步地, 如果 $r$ 阶矩 $\E{X^r}$ 存在, 则对于任意 $k > 0$, $$\P{X \ge k} \le \min_{j \le r}\frac{\E{X^j}}{k^j}$$
\end{theorem}
\begin{theorem}[Chebyshev Inequality]
	如果随机变量 $X$ 方差存在, 则对于任意 $\varepsilon > 0$, $$\P{|X - \E{X}| \ge \varepsilon} \le \frac{\Var{X}}{\varepsilon^2}$$
\end{theorem}
\begin{definition}[矩生成函数, Moment Generating Function, MGF]
	如果随机变量$X$的任意 $n \in \mathbb N$ 阶矩存在, 则定义其矩生成函数为 $$M_X(t) = \E{\e^{tX}} = \sum_{i \ge 0}t^i\frac{\E{X^i}}{i!}$$
\end{definition}
\begin{theorem}[Chernoff Inequality]
	$$\P{X \ge k} \le \inf_{t > 0}\e^{-tk}M_X(t)$$
\end{theorem}

接下来我们提出三个逐渐增强的定理, 从而最终证明 Chernoff Bound.

\begin{theorem}
	$X_1, X_2, \cdots, X_n \sim \text{ i.i.d. } \mathcal B(1, p)$, 对于任意 $\varepsilon > 0$, $$\P{\frac1n\sum_{i=1}^nX_i - p \ge \varepsilon} \le \e^{-nD_B(p + \varepsilon \| p)}$$
	其中 $D_B(p \| q)$ 是两个 Bernoulli distribution $P = (p, 1 - p), Q = (q, 1 - q)$ 之间的相对熵.
	\label{chernoff-bound-1}
\end{theorem}
\begin{proof}
	\begin{equation*}
		\begin{split}
			\P{\frac1n \sum_{i=1}^{n}X_i - p \ge \varepsilon} &=\P{\sum_{i=1}^{n}X_i \ge n(p + \varepsilon)}\\
			&\le \inf_{t > 0}\e^{-tn(p + \varepsilon)}\E{\e^{t\sum_{i=1}^nX_i}}\\
			&= \inf_{t > 0}\e^{-tn(p + \varepsilon)}\prod_{i=1}^{n}\E{\e^{tX_i}}\\
			&= \inf_{t > 0}\e^{-tn(p + \varepsilon)}(p\e^t + 1 - p)^n\\
			&= \inf_{t > 0}\left(\frac{p\e^t + 1 - p}{\e^{t(p + \varepsilon)}}\right)^n\\
		\end{split}
	\end{equation*}
	通过“简单”求导, 取 $t = \ln\frac{(1 - p)(p + \varepsilon)}{p(1 - p - \varepsilon)}$ 时上式右边取最小值, 从而有
	\begin{equation*}
		\begin{split}
			\P{\frac1n \sum_{i=1}^{n}X_i - p \ge \varepsilon} &\le \left(\frac{\frac{(1-p)(p+\varepsilon)}{1-p-\varepsilon}+1-p}{\left(\frac{(1-p)(p+\varepsilon)}{p(1-p-\varepsilon)}\right)^{p+\varepsilon}}\right)^n = \left(\frac{\frac{1-p}{1-p-\varepsilon}}{\left(\frac{(1-p)(p+\varepsilon)}{p(1-p-\varepsilon)}\right)^{p+\varepsilon}}\right)^n \\&= \left(\left(\frac{p}{p+\varepsilon}\right)^{p+\varepsilon}\left(\frac{1-p}{1-p-\varepsilon}\right)^{1-p-\varepsilon}\right)^n = \e^{-nD_B(p+\varepsilon \| p)}
		\end{split}
	\end{equation*}
\end{proof}

\begin{theorem}
	$X_1, X_2, \cdots, X_n \in [0, 1]$ 是 $n$ 个期望相同的独立随机变量, $\E{X_i} = p$, 对于任意 $\varepsilon > 0$, $$\P{\frac1n\sum_{i=1}^nX_i - p \ge \varepsilon} \le \e^{-nD_B(p + \varepsilon \| p)}$$
	\label{chernoff-bound-2}
\end{theorem}
\begin{proof}
	注意到指数函数是下凸的, 根据 Jensen Inequality, 有 $$\E{\e^{tX}} \le \E{X\e^t + (1 - X)\e^0} = p\e^t + 1 - p$$ 从而 $$\E{\e^{t\sum_{i=1}^{n}X_i}} \le (p\e^t + 1 - p)$$ 沿用\cref{chernoff-bound-1} 的证明即可.
\end{proof}
\begin{theorem}[Chernoff Bound]
	$X_1, X_2, \cdots, X_n \in [0, 1]$ 是 $n$ 个独立随机变量, $\E{X_i} = p_i$, 记 $p = \frac{1}{n} \sum_{i=1}^{n}p_i$, 对于任意 $\varepsilon > 0$, $$\P{\frac1n\sum_{i=1}^nX_i - p \ge \varepsilon} \le \e^{-nD_B(p + \varepsilon \| p)}$$
	\label{chernoff-bound-3}
\end{theorem}
\begin{proof}
	注意到对数函数是上凸的, 从而函数 $f(x) = \ln(x\e^t + 1 - x)$ 也是上凸的, 同样根据 Jensen Inequality, 有 $$\frac1n \sum_{i=1}^{n}\ln(p_i\e^t + 1 - p_i) \le \ln(p\e^t + 1 - p)$$
	从而 $$\E{\e^{t\sum_{i=1}^{n}X_i}} \le \prod_{i=1}^{n}(p_i\e^t + 1 - p_i) \le (p\e^t + 1 - p)^n$$
\end{proof}
\begin{theorem}[Additive Chernoff Bound]
	$X_1, X_2, \cdots, X_n \in [0, 1]$ 是 $n$ 个独立随机变量, $\E{X_i} = p_i$, 记 $p = \frac{1}{n} \sum_{i=1}^{n}p_i$, 对于任意 $\varepsilon > 0$, $$\P{\frac1n\sum_{i=1}^nX_i - p \ge \varepsilon} \le \e^{-2n\varepsilon^2}$$
	\label{chernoff-bound-additive}
\end{theorem}
\begin{proof}
	只需要证明 $D_B(p + \varepsilon \| p) \ge 2\varepsilon^2$ 即可. 听说可以暴力求导.
\end{proof}
\begin{theorem}[Hoeffding Bound]
	$X_1, X_2, \cdots, X_n$ 是 $n$ 个独立随机变量, $X_i \in [a_i, b_i]$, 记 $p = \frac{1}{n} \sum_{i=1}^{n}\E{X_i} = \frac1n \sum_{i=1}^{n}\frac{a_i + b_i}{2}$, 对于任意 $\varepsilon > 0$, $$\P{\frac1n\sum_{i=1}^nX_i - p \ge \varepsilon} \le \e^{-\frac{2n\varepsilon^2}{\left(\frac1n\sum_{i=1}^{n}b_i-a_i\right)^2}} \le \e^{-\frac{2n^2\varepsilon^2}{\sum_{i=1}^{n}(b_i-a_i)^2}}$$
	\label{hoeffding-bound}
\end{theorem}
\begin{theorem}[McDiarmid Inequality]
	$X_1, X_2, \cdots, X_n \in \mathcal X$ 是 $n$ 个独立随机变量, 如果对于 $f: \mathcal X^n \to \mathbb R$ 存在常数 $c_1, c_2, \cdots, c_n$ 使得 $$|f(x_1, \cdots, x_i, \cdots, x_n) - f(x_1, \cdots, x_i', \cdots, x_n)| \le c_i$$对于任意 $i \in [n], x_1, \cdots, x_n, x_i'$ 成立, 则对于任意 $\varepsilon > 0$, 有 $$\P{f(x_1, \cdots, x_n) - \E{f(x_1, \cdots, x_n)} \ge \varepsilon} \le \exp\left(\frac{-2\varepsilon^2}{\sum_{i=1}^{n}c_i^2}\right)$$
	\label{mcdiarmid-inequ}
\end{theorem}
\begin{theorem}[Draw with/without Replacement]
	有 $m$ 个数 $a_1, \cdots, a_m \in \{0, 1\}$, 记 $p = \frac1m \sum\limits_{i=1}^{m}a_i$. $X_1, \cdots, X_n$ 为从 $\{a_1, \cdots, a_m\}$ 中的随机放回抽样, $Y_1, \cdots, Y_n$ 为从 $\{a_1, \cdots, a_m\}$ 中的随机不放回抽样, 则对于任意 $\varepsilon > 0$ 有
	$$\P{\frac1n \sum_{i=1}^{n}X_i - p \ge \varepsilon} \le \e^{-2n\varepsilon^2}, \qquad \P{\frac1n \sum_{i=1}^{n}Y_i - p \ge \varepsilon} \le \e^{-2n\varepsilon^2}$$
	\label{draw-w-replacement}
\end{theorem}
\begin{proof}
	对于随机放回抽样, 显然每次抽样是独立的, 从而结论是 Chernoff Bound 的平凡推论.

	对于随机不放回抽样, 注意到 $\E{\prod_{i \in I}Y_i} \le \E{\prod_{i \in I}X_i}$ 对任意指标集 $I \subseteq \{1, \cdots, n\}$ 成立, 从而可以证明 $\E{\e^{t\sum_{i=1}^nY_i}} \le \E{\e^{t\sum_{i=1}^nX_i}}$.
\end{proof}
\newpage
\section{VC Theory}
对一个分类器 $f$, 通常有两种评价指标: training error $err_S(f) = \mathbb P_{(x, y) \in S}[y \neq f(x)]$ 与 generalization error $err_D(f) = \mathbb P_{(x, y) \sim D}[y \neq f(x)]$. 接下来可能会不加声明地用 $S$ 表示从数据集 $D$ 中 sample 出来的训练集.

称 $err_D(f) - err_S(f)$ 为分类器 $f$ 的 generalization gap. 我们提出\obj{一致收敛(uniformly converge)}的概念, 它表示随着训练集 $S$ 的增大, hypothesis space $\mathcal F$ 中的所有分类器 $f$ 的 generalization gap 都会“一致”地被 bound 住.

\begin{theorem}[Uniform Convergence when $|\mathcal F| < \infty$]
	$S$ 是从数据集 $D$ 中随机采样的训练集, $|S| = n$, 有 $$\P{\forall f \in \mathcal F, err_D(f) - err_S(f) \ge \varepsilon} \le |\mathcal F|\e^{-2n\varepsilon^2}$$
\end{theorem}
\begin{proof}
	对于某个确定的 $f \in \mathcal F$, 注意到 $err_S(f) = \frac1n \sum_{i=1}^n[y_i \neq f(x_i)]$, $\E{y_i \neq f(x_i)} = err_D(f)$, 故根据 Chernoff Bound 有 $\P{err_D(f) - err_S(f) \ge \varepsilon} \le \e^{-2n\varepsilon^2}$. 再结合 Union Bound 即得结论.
\end{proof}
\begin{theorem}[VC Theorem]
	对于 VC-dimension (会在接下来定义)为 $d$ 的 hypothesis space $\mathcal F$, 从数据集 $D$ 中随机采样大小为 $n$ 的训练集 $S$, 则 $$\P{\sup_{f \in \mathcal F}|err_D(f) - err_S(f)| \ge \varepsilon} \le 4\left(\frac{2\e n}{d}\right)^d\e^{-n\varepsilon^2/8}$$ 或者等价地, 有至少 $1 - \delta$ 的概率, 对所有 $f \in \mathcal F$ 有 $$err_D(f) \le err_S(f) + O\left(\sqrt{\frac{d\ln n + \ln (1/\delta)}{n}}\right)$$
	\label{VC-theorem}
\end{theorem}

为了接下来的叙述方便, 我们引入一些记号:
\begin{itemize}
	\item 对于分类器 $f \in \mathcal F$ 以及数据点 $z = (x, y) \sim D$, 定义 $\phi_f(z) = \mathbbm 1[y \neq f(x)]$, 即每个 $\phi_f$ 是一个“长度为$|D|$” 的 $01$ 串, $1$ 表示 $f$ 会在这一位对应的数据点上出错.
	\item 定义 $\Phi_{\mathcal F} = \{\phi_f | f \in \mathcal F\}$. 由于以下不会超过一个 hypothesis space, 故省略角标简记为 $\Phi$.
\end{itemize}

如此一来, 对于$S = \{z_1 = (x_1, y_1), \cdots, z_n = (x_n, y_n)\}$, 两种错误率 $err_S(f)$ 和 $err_D(f)$ 就分别等价于 $\frac1n \sum\limits_{i=1}^{n}\phi_f(z_i)$ 和 $\mathbb E_{z \sim D}\phi_f(z)$, 而我们需要限制的概率也变成了 $$\mathbb P_{S \sim D^n}\left[\sup_{\phi \in \Phi}\left|\frac1n\sum_{i=1}^{n}\phi(z_i) - \mathbb E_{z \sim D}[\phi(z)]\right| \ge \varepsilon\right]$$

\begin{lemma}[Double Sampling]
	取 $n \ge \frac{\ln 2}{\varepsilon^2}$, 有
	$$\mathbb P_{S \sim D^n}\left[\sup_{\phi \in \Phi}\left|\frac1n\sum_{i=1}^{n}\phi(z_i) - \mathbb E_{z \sim D}[\phi(z)]\right| \ge \varepsilon\right] \le 2\mathbb P_{S \sim D^{2n}}\left[\sup_{\phi \in \Phi}\left|\frac1n\sum_{i=1}^{n}\phi(z_i) - \frac1n\sum_{i=n+1}^{2n}\phi(z_i)\right| \ge \frac{\varepsilon}{2}\right]$$
\end{lemma}

通过 Double Sampling, 我们只需要限制 $\frac1n\sum\limits_{i=1}^{n}\phi(z_i)$ 与 $\frac1n\sum\limits_{i=n+1}^{2n}\phi(z_i)$ 的差. 考虑一种新的抽样方式, 先随机抽取 $\{z_1, \cdots, z_{2n}\}$, 再对其随机排列, 这样显然是与原先等价的, 即

$$\mathbb P_{S \sim D^{2n}}\left[\sup_{\phi \in \Phi}\left|\frac1n\sum_{i=1}^{n}\phi(z_i) - \frac1n\sum_{i=n+1}^{2n}\phi(z_i)\right| \ge \varepsilon\right] = \mathbb E_{S \sim D^n} \left[\mathbb P_{\sigma}\left[\sup_{\phi \in \Phi}\left|\frac1n\sum_{i=1}^{n}\phi(z_{\sigma(i)}) - \frac1n\sum_{i=n+1}^{2n}\phi(z_{\sigma(i)})\right| \ge \varepsilon\right]\right]$$

这么做的意义是什么? 意义是可以先只考虑内层的 $\mathbb P_{\sigma}$ 而不管 $S \sim D^n$ 的选取. 看似强行取的随机排列 $\sigma$ 是为了内层可以被 bound, 不然 $\mathbbm 1\left[\left|\frac1n\sum_{i=1}^{n}\phi(z_i) - \frac1n\sum_{i=n+1}^{2n}\phi(z_i)\right| \ge \varepsilon \right]$ 还不太方便处理.

记 $N^{\Phi}(z_1, \cdots, z_n)$ 表示 $\#\{(\phi(z_1), \cdots, \phi(z_n)) | \phi \in \Phi\}$, 即 $\Phi$ 中的所有 $01$ 串在数据点 $z_1, \cdots, z_n$ 上有多少种不同的. 从这个角度想, 其实 $\sup_{\phi \in \Phi}$ 只是在对有限项求 $\max$, 故根据 Union Bound 可以得到 $$\mathbb P_{\sigma}\left[\sup_{\phi \in \Phi}\left|\frac1n\sum_{i=1}^{n}\phi(z_{\sigma(i)}) - \frac1n\sum_{i=n+1}^{2n}\phi(z_{\sigma(i)})\right| \ge \varepsilon\right] \le N^{\Phi}(z_1, \cdots, z_{2n})\mathbb P_{\sigma}\left[\left|\frac1n\sum_{i=1}^{n}\phi(z_{\sigma(i)}) - \frac1n\sum_{i=n+1}^{2n}\phi(z_{\sigma(i)})\right| \ge \varepsilon\right]$$

\textit{其实这里写得不太严谨, 右边应该是对 $N^{\Phi}(z_1, \cdots, z_{2n})$ 个不同的 $\phi$ 分别求概率再相加, 但我们接下来会对任意 $\phi$ 限制 $\mathbb P_{\sigma}\left[\left|\frac1n\sum\limits_{i=1}^{n}\phi(z_{\sigma(i)}) - \frac1n\sum\limits_{i=n+1}^{2n}\phi(z_{\sigma(i)})\right| \ge \varepsilon\right]$, 所以应该也无伤大雅.}

对于一个特定的 $\phi \in \Phi$, 考虑 $\frac1n\sum\limits_{i=1}^{n}\phi(z_{\sigma(i)})$ 其实就是在 $\{\phi(z_1), \cdots, \phi(z_{2n})\}$ 这 $2n$ 个数中做不放回抽样, 故根据\cref{draw-w-replacement}, 有 \begin{equation*}
	\begin{split}
		\mathbb P_{\sigma}\left[\left|\frac1n\sum\limits_{i=1}^{n}\phi(z_{\sigma(i)}) - \frac1n\sum\limits_{i=n+1}^{2n}\phi(z_{\sigma(i)})\right| \ge \varepsilon\right]
		&=2\mathbb P_{\sigma}\left[\frac1n\sum\limits_{i=1}^{n}\phi(z_{\sigma(i)}) - \frac1n\sum\limits_{i=n+1}^{2n}\phi(z_{\sigma(i)}) \ge \varepsilon\right]\\
		&=2\mathbb P_{\sigma}\left[\frac1n\sum\limits_{i=1}^{n}\phi(z_{\sigma(i)}) - \frac1{2n}\sum\limits_{i=1}^{2n}\phi(z_{\sigma(i)}) \ge \frac{\varepsilon}{2}\right]\\
		&=2\mathbb P_{\sigma}\left[\frac1n\sum\limits_{i=1}^{n}\phi(z_{\sigma(i)}) - p \ge \frac{\varepsilon}{2}\right]\\
		&\le 2\e^{-\frac{n\varepsilon^2}{2}}
	\end{split}
\end{equation*}

从而我们得到了 $$\mathbb P_{S \sim D^{2n}}\left[\sup_{\phi \in \Phi}\left|\frac1n\sum_{i=1}^{n}\phi(z_i) - \frac1n\sum_{i=n+1}^{2n}\phi(z_i)\right| \ge \varepsilon\right] \le 2\e^{-\frac{n\varepsilon^2}{2}}\mathbb E_{S \sim D^{2n}}\left[N^{\Phi}(z_1, \cdots, z_{2n})\right]$$

于是我们只需要限制 \obj{Growth Function} $N^{\Phi}(n) = \max_{S \sim D^n}N^{\Phi}(z_1, \cdots, z_n)$ 即可. 除去 $N^{\Phi}(n) \equiv 2^n$ 这种平凡的情况(这种情况意味着这种场景是 somehow not learnable 的), 我们指出 $N^{\Phi}(n)$ 是多项式增长的.

\begin{lemma}
	假设 $N^{\Phi}(d + 1) < 2^{d+1}$ 成立, 则对于任意 $n \ge d + 1$, 都有 $N^{\Phi}(n) \le \sum\limits_{k=0}^{d}\binom nk$.
\end{lemma}
\begin{proof}
	$N^{\Phi}(d + 1) < 2^{d+1}$ 说明存在一种 $w \in \{0, 1\}^{d+1}$ 无法被 $\Phi$ 表示, 我们将其称为 \obj{forbidden pattern}. 对于 $n \ge d + 1$, 考虑指标集 $\mathcal I = \{i_1, i_2, \cdots, i_{d+1}\} \subseteq [n]$, 用 $w_{\mathcal I} \in \{0, 1, *\}^n$ 表示在 $\mathcal I$ 指标上填 $w$, 其余位置填通配符 $*$ 得到的 $n$ 位 $01$ 串模式. 用 $E(w_{\mathcal I})$ 表示能被 $w_{\mathcal I}$ 模式匹配的 $n$ 位 $01$ 串集合, 我们需要做的是限制 $\left|\bigcup_{\mathcal I}E(w_{\mathcal I})\right|$ 的上界(从而限制其补集的下界).

	如果 $w = 0^{d+1}$, 那么这个问题是好办的, 因为相当于 $\bigcup_{\mathcal I}E(w_{\mathcal I})$ 中包含了所有有至少 $d+1$ 个 $0$ 的 $01$ 串, 答案就恰好是 $\sum\limits_{k = d+1}^{n}\binom{n}{k}$. 如果 $w \neq 0^{d+1}$, 直观来看 $\bigcup_{\mathcal I}E(w_{\mathcal I})$ 的大小不会减小, 因此结论依然成立. 详细证明则是对于每个 $i \in [n]$, 把所有 $w_{\mathcal I}$ 在这一位上的 $1$ 变成 $0$, 验证并不会使集合大小变大.
\end{proof}
\begin{corollary}
	考虑 $\sum\limits_{k=0}^d \binom nk \le \left(\frac{\e n}{d}\right)^d = O(n^d)$, 我们最终得到了$$\mathbb P_{S \sim D^n}\left[\sup_{\phi \in \Phi}\left|\frac1n\sum_{i=1}^{n}\phi(z_i) - \mathbb E_{z \sim D}[\phi(z)]\right| \ge \varepsilon\right] \le 4\left(\frac{2\e n}{d}\right)^d\e^{-\frac{n\varepsilon^2}{8}}$$

	其中 $d = \max\{n | N^{\Phi}(n) = 2^n\}$ 被定义为 hypothesis space $\mathcal F$ 的 VC dimension.
\end{corollary}

\newpage
\section{Game Theory}

Game theory is the study of mathematical models of strategic interactions among rational agents, cited from Wikipedia.

我们引入“双人矩阵博弈”作为对博弈论最基础的介绍. 注意, 接下来我们考虑的所有问题都是零和的.

\begin{definition}[Two-player Matrix Game]
	有一个 $M \in \mathbb R^{m \times n}$ 的矩阵. 两名玩家 Alice 和 Bob 参加了这场博弈. Alice, \obj{the row player} 选择一行 $i \in [m]$, 相应的, Bob, \obj{the column player} 选择一列 $j \in [n]$, 此时 Alice 获得收益 $-M_{ij}$, Bob 获得收益 $M_{ij}$.
\end{definition}

我们首先探讨\obj{纯策略(pure strategy)}的情景, 指的是 Alice 和 Bob 必须分别选择某个确定的行或列.

当 Alice 先做出选择时, 当她选出第 $i$ 行后, 她会认为 Bob 会选择第 $j_i = \arg\max_{j}M_{ij}$ 列, 因此她会选择第 $\arg\min_i\max_jM_{ij}$ 行, 导致最终的博弈结果为 $\min_i\max_jM_{ij}$.

同理, 当 Bob 先做选择时, 他会选择第 $\arg\max_j\min_iM_{ij}$ 列, 导致最终的博弈结果为 $\max_j\min_iM_{ij}$.

我们指出在纯策略的情境下, 后手是有优势的, 即

\begin{theorem}
	$\min_i\max_jM_{ij} \ge \max_j\min_iM_{ij}$ 对于任意 $M \in \mathbb R^{m \times n}$ 都成立, 同时存在 $M'$, 使 $\min_i\max_j{M'}_{ij} > \max_j\min_i{M'}_{ij}$.	
\end{theorem}
\begin{proof}
	记 $i_0 = \arg\min_i\max_jM_{ij}, j_0 = \arg\max_j\min_iM_{ij}$, 有 $$\min_i\max_jM_{ij} = \max_jM_{i_0j} \ge M_{i_0j_0} \ge \min_iM_{ij_0} = \max_j\min_iM_{ij}$$

	考虑 $M' = \begin{pmatrix}
		-1 & 1 \\ 1 & -1
	\end{pmatrix}$, 有 $\min_i\max_j{M'}_{ij} = -1, \max_j\min_i{M'}_{ij} = 1$.
\end{proof}

接着我们研究\obj{混合策略(mixed strategy)}, 其意味着玩家做出的决策可以不是确定的行列选择, 而是一个概率分布. 相应地, 得到的收益也就变成了期望收益.

形式化地, Alice 选择给出概率分布 $p = (p_1, \cdots, p_m) \in [0, 1]^m$, Bob 给出概率分布 $q = (q_1, \cdots, q_n) \in [0, 1]^n$. 合法的概率分布需要满足 $\|p\|_1 = \|q\| = 1$, 而此时两人的收益也分别是 $-p^{\text T}Mq$ 与 $p^{\text T}Mq$.

与纯策略的情境同理, 当 Alice 先手时, 博弈结果为 $\min_{p \in [0, 1]^m, \|p\|_1 = 1}\max_{q \in [0, 1]^n, \|q\|_1 = 1} p^{\text T}Mq$, 当 Bob 先手时, 博弈结果为 $\max_{q \in [0, 1]^n, \|q\|_1 = 1}\min_{p \in [0, 1]^m, \|p\|_1 = 1} p^{\text T}Mq$. 在接下来的叙述中, 我们默认 $p, q$ 应取合法的概率分布, 而忽略在 $\min, \max$ 记号下的明确限制.

我们想要知道混合策略下后手还有没有优势. John von Neuman 告诉我们, 没有.

\begin{theorem}[von Neuman Minimax Theorem]
	$$\min_{ p}\max_{ q}  p^{\text T}M q = \max_{ q}\min_{ p}  p^{\text T}M q$$
	\label{minimax}
\end{theorem}

\newpage
\section{Lagrange Duality, Linear Seperation, SVM and more}
开摆了.
\newpage
\section{Boosting}

Boosting 做的事情就是把一堆 classifier 放在一起, 从而得到一个更准确的 classifier.

以下算法中, $\gamma$-weak learning algorithm 表示其可以对于任意输入的带权训练集 $D$, 都能给出一个 classifier, 能在至少 $\frac{1 + \gamma}{2}$ 的数据上得到正确的结果.

\begin{algorithm}
	\caption{AdaBoost}
	\begin{algorithmic}[1]
		\Require training set $S = \{(x_1, y_1), \cdots, (x_n, y_n)\}$, $\gamma$-weak learning algorithm $\mathcal A$
		\State $D_1(i) \gets \frac1n, \forall i \in [n]$.
		\For{$t = 1 \to T$}
			\State Use $\mathcal A$ to learn a classifier $h_t$ based on $D_t$.
			\State $\varepsilon_t \gets \sum\limits_{i=1}^{n}D_t(i)\mathbbm 1[y_i \neq h_t(x_i)]$
			\State $\gamma_t \gets 1 - 2\varepsilon_t$ \Comment{$\gamma_t \ge \gamma$}
			\State $\alpha_t \gets \frac12 \ln\frac{1 + \gamma_t}{1 - \gamma_t}$
			\State $Z_t \gets \sum_i D_t(i)\exp(-y_i\alpha_th_t(x_i)) \left(= 2\sqrt{\varepsilon_t (1 - \varepsilon_t)}\right)$
			\State $D_{t + 1}(i) \gets \frac{1}{Z_t}D_t(i)\exp(-y_i\alpha_th_t(x_i))$
		\EndFor
		\State \Return a classifier $F$, $F(x) = \text{sgn}\left(\sum\limits_{t=1}^{T}\alpha_th_t(x)\right) = \text{sgn}(f(x))$
	\end{algorithmic}
\end{algorithm}

我们陈述以下命题:
\begin{enumerate}
	\item $\alpha_t = \arg\min\limits_{\alpha}Z_t = \arg\min\limits_{\alpha} \sum\limits_{i=1}^{n}D_t(i)\exp(-\alpha y_i h_t(x_i))$.
	\item $\prod\limits_{t=1}^{T}Z_t = \frac1n \sum\limits_{i=1}^{n}\exp\left(-y_i\sum\limits_{t=1}^{T}\alpha_t h_t(x_i)\right) = \frac1n \sum\limits_{i=1}^{n}\exp\left(-y_if(x_i)\right)$.
	\item $\sum\limits_{i=1}^{n}D_{t+1}(i)\mathbb I[y_i \neq h_t(x_i)] = \frac12$. (与证明最终结果无关, 在此是为了指明每一轮中训练得到的 $h_t$ 的优化方向是 orthogonal 的.)
	\item $\mathbb P_{(x_i, y_i) \sim S} \left[F(x_i) \neq y_i\right] \le (1 - \gamma^2)^{T/2}$.
\end{enumerate}

注意到 $\mathbb P_{(x_i, y_i) \sim S} \left[F(x_i) \neq y_i\right]$ 的最小非零结果应为 $\frac1n$, 故我们只需要限制 $(1 - \gamma^2)^{T/2} < \frac1n$, 即 $T = \Omega(\log n)$, 就能将 error 降为 $0$.

\begin{enumerate}
	\item Recall that $\varepsilon_t = \sum\limits_{i=1}^{n}D_t(i)\mathbb I[y_i \neq h_t(x_i)]$, by applying \obj{AM–GM inequality} we have
	$$Z_t = \sum\limits_{i=1}^{n}D_t(i)\exp(-\alpha y_i h_t(x_i)) = \varepsilon_t \exp(\alpha) + (1 - \varepsilon)\exp(-\alpha) \ge 2\sqrt{\varepsilon_t(1 - \varepsilon_t)}$$
	where the equality holds if and only if $$\varepsilon_t \e^{\alpha} = (1 - \varepsilon_t)\e^{-\alpha} \Leftrightarrow \alpha = \frac12 \ln\frac{1 - \varepsilon_t}{\varepsilon_t} = \frac12 \ln\frac{1 + \gamma_t}{1 - \gamma_t} = \alpha_t$$
	where $\gamma_t = 1 - 2\varepsilon$ in the assignment of $\alpha_t$. This suggests that $\alpha_t = \arg\min\limits_{\alpha} Z_t$ as desired.

	\item Notice that $D_{t+1}(i) = \frac{D_t(i)\exp(-\alpha_t y_i h_t(x_i))}{Z_t}$, by iteratively substitute the term of $D_t(i)$ in the expression of $Z_T$ ($Z_T = \sum_{i=1}^{n}D_T(i)\exp(-\alpha_T y_i h_T(x_i))$), we can eventually obtain the following equality. \begin{equation*}
		\begin{split}
			\prod_{t=1}^{T}Z_t &= \prod_{t=1}^{T-1}Z_t \sum_{i=1}^{n}D_T(i)\exp(-\alpha_T y_i h_T(x_i))\\
			&= \prod_{t=1}^{T-2}Z_t \sum_{i=1}^{n}D_{T-1}(i)\exp(-\alpha_T y_i h_T(x_i) -\alpha_{T-1} y_i h_{T-1}(x_i))\\
			&= \cdots\\
			&= \sum\limits_{i=1}^{n}D_0(i)\exp\left(-y_i\sum\limits_{t=1}^{T}\alpha_t h_t(x_i)\right)\\
			&= \frac1n \sum\limits_{i=1}^{n}\exp\left(-y_if(x_i)\right)
		\end{split}
	\end{equation*}
	
	\item \begin{equation*}
		\begin{split}
			\sum_{i=1}^{n}D_{t+1}(i)\mathbb I[y_i \neq h_t(x_i)] &= \sum_{i=1}^{n}\frac{D_{t}(i)\exp(-\alpha_t y_i h_t(x_i))}{Z_t}\mathbb I[y_i \neq h_t(x_i)]\\
			&= \frac{\sum\limits_{i=1}^{n}D_{t}(i)\exp(\alpha_t)\mathbb I[y_i \neq h_t(x_i)]}{\sum\limits_{i=1}^{n}D_{t}(i)\exp(\alpha_t)\mathbb I[y_i \neq h_t(x_i)] + \sum\limits_{i=1}^{n}D_{t}(i)\exp(-\alpha_t)\mathbb I[y_i = h_t(x_i)]}\\
			&= \frac{\varepsilon_t \e^{\alpha_t}}{\varepsilon_t \e^{\alpha_t} + (1 - \varepsilon_t)\e^{-\alpha_t}}
		\end{split}
	\end{equation*}

	Since $\alpha_t$ is chosen so that $\varepsilon_t \e^{\alpha_t} = (1 - \varepsilon_t)\e^{-\alpha_t}$, we can prove that $\sum\limits_{i=1}^{n}D_{t+1}(i)\mathbb I[y_i \neq h_t(x_i)] = \frac12$.

	\item \begin{align*}
		\begin{split}
			\mathbb P_{(x_i, y_i)\sim S}[F(x_i) \neq y_i] &= \frac1n \sum_{i=1}^n \mathbbm 1[y_if(x_i) \le 0] \le \frac1n \sum_{i=1}^{n} \exp(-y_if(x_i)) = \prod_{t=1}^T Z_t \\
			&= \prod_{t=1}^T 2\sqrt{\varepsilon_t(1 - \varepsilon_t)} 
			= \prod_{t=1}^T \sqrt{1 - \gamma_t^2} 
			\le (1 - \gamma^2)^{T/2}
		\end{split}
	\end{align*}
\end{enumerate}

\newpage
\section{PAC-Bayesian Theory}
\begin{theorem}[PAC-Bayesian Theorem]
	对于给定的 prior distribution of classifiers $\mathcal P$, 从数据集 $D$ 中随机抽取大小为 $n$ 的训练集 $S$, 有至少 $1 - \delta$ 的概率, 对于任意 distribution of classifiers $\mathcal Q$ 有如下不等式成立
	$$\mathbb E_{h \sim \mathcal Q}[err_D(h)] \le \mathbb E_{h \sim \mathcal Q}[err_S(h)] + \sqrt{\frac{D_{KL}(\mathcal Q \| \mathcal P) + \log(3 / \delta)}{n}}$$
	其中 $err_{X}(f)$ 表示 classifier $f$ 在数据集 $X$ 上的错误率, 即 $\mathbb P_{(x, y) \in X}[y \neq f(x)]$, $D_{KL}(\mathcal Q \| \mathcal P) = \mathbb E_{h \sim \mathcal Q}\left[\ln \frac{\mathcal Q_h}{\mathcal P_h}\right]$ 为概率分布 $\mathcal Q$ 与 $\mathcal P$ 的 KL 散度.
	\label{PAC-Bayesian}
\end{theorem}

\begin{lemma}
	对于任意在 hypothesis space $\mathcal F$ 上的概率分布 $\mathcal P, \mathcal Q$, 以及任意函数 $f: \mathcal F \to \mathbb R$, 都有
	$$\mathbb E_{h \sim Q}[f(h)] \le \ln \mathbb E_{h' \sim P}[\exp(f(h'))] + D_{KL}(\mathcal Q \| \mathcal P)$$
	\label{PAC-Bayesian-lem-1}
\end{lemma}
\begin{proof}
	\begin{equation*}
		\begin{split}
			\text{RHS} -\text{LHS} &= \ln \mathbb E_{h' \sim P}[\exp(f(h'))] + D_{KL}(\mathcal Q \| \mathcal P) - \mathbb E_{h \sim Q}[f(h)]\\
			&= \ln \mathbb E_{h' \sim P}[\exp(f(h'))] + \mathbb E_{h \sim \mathcal Q}\left[\ln \frac{\mathcal Q_h}{\mathcal P_h}\right] - \mathbb E_{h \sim Q}[f(h)]\\
			&= \mathbb E_{h \sim Q} \left[\ln \frac{\mathcal Q_h}{\frac{\mathcal P_h \exp(f(h))}{\mathbb E_{h' \sim \mathcal P} [\exp(f(h'))]}}\right]\\
			&= \mathbb E_{h \sim Q} \left[\ln \frac{\mathcal Q_h}{\mathcal R_h}\right]\\
			&= D_{KL}(\mathcal Q \| \mathcal R)\\
			& \ge 0
		\end{split}
	\end{equation*}
	其中 $\mathcal R$ 也是一个 $\mathcal F$ 上的概率分布, $\mathcal R_h = \frac{\mathcal P_h \exp(f(h))}{\mathbb E_{h' \sim \mathcal P} [\exp(f(h'))]}$.
\end{proof}

\begin{lemma}
	对于任意 $\delta > 0$, 有 $$\mathbb P_{S \sim D^n} \left(\mathbb E_{h \sim \mathcal P} [ \e^{n(err_D(h) - err_S(h))^2} ] \ge 3/\delta\right) \le \delta$$
	\label{PAC-Bayesian-lem-2}
\end{lemma}
\begin{proof}
	先证明对于某个固定的 $h \sim \mathcal P$, 有 $$\mathbb E_{S \sim D^n}[\e^{n(err_D(h) - err_S(h))^2}] \le 3$$

	记 $\Delta = |err_D(h) - err_S(h)|$, 根据 Chernoff bound, 有 $$\mathbb P_{S \sim D^n}(\Delta \ge \varepsilon) \le 2\exp(-2n\varepsilon^2)$$

	于是\begin{equation*}
		\begin{split}			
			\mathbb E_{S \sim D^n} \left[\e^{n\Delta^2}\right] &= \int_0^{+\infty}\mathbb P_{S \sim D^n}\left(\e^{n\Delta^2} \ge t\right) \text dt\\
			&= \int_1^{+\infty}\mathbb P_{S \sim D^n}\left(\Delta \ge \sqrt{\frac{\ln t}{n}}\right) \text dt + 1\\
			&\le \int_1^{+\infty}2\e^{-2\ln t}\text dt + 1\\
			&= 3
		\end{split}
	\end{equation*}

	随后, 使用 Markov Inequality 得到\begin{equation*}
		\begin{split}
			\mathbb P_{S \sim D^n} \left(\mathbb E_{h \sim \mathcal P} [ \e^{n\Delta^2} ] \ge 3/\delta\right) &\le \frac{\mathbb E_{S \sim D^n} \left(\mathbb E_{h \sim \mathcal P} [ \e^{n\Delta^2} ]\right)}{3/\delta} = \frac{ \mathbb E_{h \sim \mathcal P}\left(\mathbb E_{S \sim D^n} [ \e^{n\Delta^2} ]\right)}{3/\delta} \le \frac{ \mathbb E_{h \sim \mathcal P}\left(3\right)}{3/\delta} = \delta
		\end{split}
	\end{equation*}
\end{proof}

我们利用上述两个引理证明\cref{PAC-Bayesian}. 有至少 $1 - \delta$ 的概率, 
\begin{equation*}
	\begin{split}
		\left(\mathbb E_{h \sim \mathcal Q}[err_D(h) - err_S(h)]\right)^2
		& \le \mathbb E_{h \sim \mathcal Q}[\Delta^2]\\
		&= \frac1n \mathbb E_{h \sim \mathcal Q}[n\Delta^2]\\
		&\le \frac1n \left(\ln \mathbb E_{h \sim P}\left[\e^{n\Delta^2}\right] + D_{KL}(\mathcal Q \| \mathcal P)\right)\\
		&\le \frac1n \left(\ln(3/\delta) + D_{KL}(\mathcal Q \| \mathcal P)\right)
	\end{split}
\end{equation*}
其中第一行等号使用了 Cauchy Inequality, 第三行使用了\cref{PAC-Bayesian-lem-1} 代入 $f(h) = n\Delta^2$, 第四行使用了\cref{PAC-Bayesian-lem-2}, with probability at least $1 - \delta$.

\subsection{PAC-Bayesian Bound for SVM}
\begin{proposition}
	对于任意的 distribution of classifiers $\mathcal Q$, 令 $g_{\mathcal Q}$ 为一个确定性二分类器, $g_{\mathcal Q}(x) = \text{sgn}(\mathbb E_{h \sim \mathcal Q}h(x))$, 则 $$err_D(g_{\mathcal Q}) \le 2\mathbb E_{h \sim \mathcal Q}[err_D(h)]$$
\end{proposition}
\begin{proof}
	如果 $g_{\mathcal Q}$ 在一个数据点 $x$ 上出错, 则说明 $\mathcal Q$ 中至少一半的 classifier 都在 $x$ 上出错.
\end{proof}

考虑两个 distribution of classifiers $\mathcal P = \mathcal N(\mathbf 0, I_d), \mathcal Q = \mathcal N(\mu\mathbf w, I_d)$, 其中 $\|\mathbf w\|_2 = 1$, $\mu$ 是缩放系数. 此时 $g_{\mathcal Q}$ 就是传统理解下的 linear classifier $\mathbf w$ (这里不考虑常数 $b$).

根据\cref{PAC-Bayesian} 的结论, 我们有 $$err_D(g_{\mathcal Q}) \le 2 \left[\mathbb E_{h \sim \mathcal Q}err_S(h) + \sqrt{\frac{D_{KL}(\mathcal Q \| \mathcal P) + \log(3 / \delta)}{n}}\right]$$

\begin{equation*}
	\begin{split}
		D_{KL}(\mathcal Q \| \mathcal P)
		&= \int_{\mathbb R^d}\frac{1}{(2\pi)^{d/2}}\exp\left[-\frac12\|\mathbf x - \mu\mathbf w\|^2\right]\frac12\left(\|\mathbf x\|^2 - \|\mathbf x - \mu\mathbf w\|^2\right)\text d\mathbf x\\
		&= \int_{\lambda}\int_{\mathbf y \in \mathbb R^{d-1}, \mathbf y \perp \mathbf w}\frac{1}{(2\pi)^{d/2}}\exp\left[-\frac12\|\lambda\mathbf w + \mathbf y - \mu\mathbf w\|^2\right]\frac12\left(\|\lambda\mathbf w + \mathbf y\|^2 - \|\lambda\mathbf w + \mathbf y - \mu\mathbf w\|^2\right)\text d\lambda\text d\mathbf y\\
		&= \int_{\lambda}\int_{\mathbf y \in \mathbb R^{d-1}, \mathbf y \perp \mathbf w}\frac{1}{(2\pi)^{d/2}}\exp\left[-\frac12(\lambda - \mu)^2 - \frac12 \|\mathbf y\|^2\right]\frac12\left(\lambda^2 + \|\mathbf y\|^2 - (\lambda - \mu)^2 - \|\mathbf y\|^2\right)\text d\lambda\text d\mathbf y\\
		&= \int_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi}}\exp\left[-\frac12(\lambda - \mu)^2\right]\frac12(2\lambda\mu - \mu^2)\text d\lambda\left[\int_{\mathbf y \in \mathbb R^{d-1}, \mathbf y \perp \mathbf w}\frac{1}{(2\pi)^{(d-1)/2}}\exp\left(-\frac12 \|\mathbf y\|^2\right)\text d\mathbf y\right]\\
		&= \int_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi}}\exp\left[-\frac12(\lambda - \mu)^2\right](\lambda\mu - \mu^2)\text d\lambda + \frac{\mu^2}{2}\\
		&=\int_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi}}\exp\left[-\frac12(\lambda - \mu)^2\right]\mu\text d\frac{(\lambda - \mu)^2}{2} + \frac{\mu^2}{2}\\
		&= \frac{\mu^2}{2}
	\end{split}
\end{equation*}












\newpage
\section{Algorithmic Stability}
\begin{definition}[一致稳定, Uniform Stability]
	$\mathcal A$ 是输入训练集 $S = (z_1, \cdots, z_n)$, 输出一个分类器 $\mathcal A(S)$ 的学习算法. 记 $S^i = (z_1, \cdots, z_{i-1}, z_i', z_{i+1}, \cdots, z_n)$ 是与 $S$ 只相差第 $i$ 个数据点的相邻训练集, $\ell (\cdot, \cdot)$ 是损失函数, 即 $\ell(f, z)$ 是在分类器 $f$ 下, 数据点 $z$ 产生的损失.

	称学习算法 $\mathcal A$ 关于 $\ell(\cdot, \cdot)$ 满足 $\beta(n)$-一致稳定性, 如果对于任意大小为 $n$ 的训练集 $S$ 及其相邻训练集 $S^i$, 以及任意数据点 $z$, 都有 $$|\ell(\mathcal A(S), z) - \ell(\mathcal A(S^i), z)| \le \beta(n)$$
\end{definition}
\begin{definition}[Risk \& Empirical Risk]
	分别类似于 test error 与 training error, 定义 risk 与 empirical risk 为
	\begin{equation*}
		\begin{split}			
			R(\mathcal A(S)) &= \mathbb E_{z \sim D}[\ell(\mathcal A(S), z)]\\
			R_{emp}(\mathcal A(S)) &= \frac1n \sum_{i=1}^{n}\ell(\mathcal A(S), z_i)
		\end{split}
	\end{equation*}
	以下讨论中不会出现超过一个学习算法, 故简记 $\Phi(S) = R(\mathcal A(S)) - R_{emp}(\mathcal A(S))$.
\end{definition}
\begin{theorem}[一致稳定能说明泛化]
	对于一个关于 $\ell(\cdot, \cdot)$ 满足 $\beta(n)$-一致稳定性的学习算法 $\mathcal A$, 其中 $|\ell(\cdot, \cdot)| \le M$ 有上界, 有 $$\P{\Phi(S) \le \varepsilon + \beta(n)} \le \exp\left( -\frac{n\varepsilon^2}{2(n\beta(n) + M)^2} \right)$$ 或者等价的, 有至少 $1 - \delta$ 的概率下式成立 $$R(\mathcal A(s)) \le R_{emp}(\mathcal A(s)) + \beta(n) + (n\beta(n) + M)\sqrt{\frac{2\ln(1/\delta)}{n}}$$
\end{theorem}
\begin{proof}
	先证明两个引理.
	\begin{lemma}
		假设 $\mathcal A$ 是对称的, 即对于任意 $n$ 元置换 $\sigma$, 有 $\mathcal A(\{z_1, \cdots, z_n\}) = \mathcal A(\{z_{\sigma_1}, \cdots, z_{\sigma_n}\})$, 则 $$\mathbb E_{S}[\Phi(S)] \le \beta(n)$$
		\label{uni-stab-lem-1}
	\end{lemma}
	\begin{proof}
		$$\mathbb E_{S}[\Phi(S)] = \mathbb E_{S, z}[\ell(\mathcal A(S), z)] - \mathbb E_{S}[\ell(\mathcal A(S), z_1)] = \mathbb E_{S, S^1}[\ell(\mathcal A(S^1), z_1) - \ell(\mathcal A(S), z_1)] \le \beta(n)$$
	\end{proof}
	\begin{lemma}
		如果 $|\ell(\cdot, \cdot)| \le M$ 有上界, 则对于任意 $S, S^i$, 有 $$|\Phi(S) - \Phi(S^i)| \le 2\left(\beta(n) + \frac Mn \right)$$
		\label{uni-stab-lem-2}
	\end{lemma}
	\begin{proof}
		除了 $\ell(\mathcal A(S), z_i) - \ell(\mathcal A(S^i), z_i')$ 一项外, 其余所有项都可以被 $\beta(n)$-稳定性限制住.
		\begin{equation*}
			\begin{split}
				|\Phi(S) - \Phi(S^i)| & = |R(\mathcal A(S)) - R_{emp}(\mathcal A(S)) - R(\mathcal A(S^i)) + R_{emp}(\mathcal A(S^i))|\\
				& \le |R_{emp}(\mathcal A(S)) - R_{emp}(\mathcal A(S^i))| + |R(\mathcal A(S)) - R(\mathcal A(S^i))|\\
				&= \frac1n |\ell(\mathcal A(S), z_i) - \ell(\mathcal A(S^i), z_i')| + \frac1n \sum_{j \neq i}|\ell(\mathcal A(S), z_j) - \ell(\mathcal A(S^i), z_j)| + \left|\mathbb E_{z \sim D}[\ell(\mathcal A(S), z) - \ell(\mathcal A(S^i), z)]\right|\\
				&\le \frac{2M}{n} + \frac{n-1}{n}\beta(n) + \beta(n)\\
				&\le 2\left(\beta(n) + \frac Mn\right)
			\end{split}
		\end{equation*}
	\end{proof}

	考虑 McDiarmid Inequality (\cref{mcdiarmid-inequ}), 把 $\Phi$ 视作一个关于 $z_1, \cdots, z_n$ 的多元函数, 则\cref{uni-stab-lem-1} 与\cref{uni-stab-lem-2} 分别给出了 $\Phi$ 的期望以及在相邻输入上的差的上界. 于是 $$\P{\Phi(S) \ge \beta(n) + \varepsilon} \le \P{\Phi(S) - \E{\Phi(S)} \ge \varepsilon} \le \exp\left(-\frac{2n\varepsilon^2}{\sum_{i=1}^nc_i^2}\right) = \exp\left(-\frac{n\varepsilon^2}{2(n\beta(n)+M)^2}\right)$$
\end{proof}

\newpage
\section{Unsupervised Learning}

前面讨论的都是监督学习. 现在我们讨论一下无监督学习.

无监督学习其实主要在做两件事情: Clustering, 以及 Dimensionality Reduction.

\subsection{Clustering}
对于一组 $\mathbf x_1, \cdots, \mathbf x_n \in \mathbb R^d$, 需要把这些数据点划分成 $k$ 个 cluster $S_1, \cdots, S_k$.

可以如下定义一种划分的损失函数: 记 $\mu_i = \frac{1}{|S_i|}\sum_{j \in S_i}\mathbf x_j$ 为第 $i$ 个 cluster 的中心, 损失函数为 $$L(\{S_1, \cdots, S_k\}) = \sum_{i=1}^{k}\sum_{j \in S_i} \| \mathbf x_j - \mu_i \|^2$$

\subsubsection{K-means}
\begin{algorithm}
	\caption{K-means}
	\begin{algorithmic}[1]
		\State Choose $k$ points as cluster centers $\mu_1, \cdots, \mu_k$ uniformly at random.
		\Repeat
			\State $S_i \gets \{j : \|\mathbf x_j - \mu_i\|^2 \le \|\mathbf x_j - \mu_k\|^2, \forall k \in [m]\}$
			\State $\mu_i \gets \frac{1}{|S_i|} \sum_{j \in S_i} \mathbf x_j$
		\Until{$k$ cluster centers do not change}
		\State \Return $\{\mu_1, \cdots, \mu_k\}$
	\end{algorithmic}
\end{algorithm}

\subsubsection{K-means++}
\begin{algorithm}
	\caption{K-means++}
	\begin{algorithmic}[1]
		\State 	Choose a point as the cluster center $\mu_1$ uniformly at random.
		\For{$i: 2 \to n$}
			\State Choose a point as the cluster center $\mu_i$, with probability proportional to $\min\limits_{1 \le k < i}\|\mathbf x_j - \mu_k\|^2$.
		\EndFor
		\State \Return $\{\mu_1, \cdots, \mu_k\}$
	\end{algorithmic}
\end{algorithm}
\begin{theorem}
	K-means++ 算法给出的损失 $L$ 与最优解 $L_{opt}$ 满足 $$\E{L} \le 8(\ln k + 2)L_{opt}$$
\end{theorem}
\subsection{Dimensionality Reduction}
wlw不讲了.

\newpage
\section{Online Learning}
在在线学习的设定下, 数据是以流的形式给出的, 在每次得到一个数据点之后, 都需要以恰当的方式更新预测器, 以优化将来的预测.

相比监督学习, 在线学习主要区别在于: \num{1} 不再区分 training 与 test, \num{2} 没有对数据的分布假设, 因而不存在 generalization 的概念. 相应的, mistake model 以及 regret 的概念会被用于衡量在线学习算法的表现效果.

\subsection{Online Learning with Expert Advice}

有 $n$ 位专家. 预测会持续 $T$ 轮, 每轮中每位专家都会给出各自的预测 $\tilde{y_{t, i}} \in \{0, 1\}$, 学习者需要根据此前得到的所有信息给出预测 $\tilde{y_t} \in \{0, 1\}$, 同时也会获得正确结果 $y_t \in \{0, 1\}$. 学习者的目标是让自己的预测结果与最好的专家尽量接近, 即最小化 $\sum\limits_{t=1}^{T}\mathbbm 1[\tilde{y_t} \neq y_t]$ 与 $\min\limits_{i \in [n]}\sum\limits_{t=1}^{T}\mathbbm 1[\tilde{y_{t, i}} \neq y_t]$ 的差(这就是regret).

\subsubsection{Weighted Majority Vote}
\begin{algorithm}
	\caption{Weighted Majority Vote}
	\begin{algorithmic}[1]
		\State Initialize $w_{1, i} \gets 1, \forall i \in [n]$
		\State Choose parameter $\beta \in (0, 1)$
		\For{$t = 1 \to T$}
			\State Make the Weighted Majority Vote $\tilde{y_t} = \begin{cases}
				0, & \sum\limits_{\tilde{y_{t, i}} = 0} >  \sum\limits_{\tilde{y_{t, i}} = 1}\\
				1, & \text{otherwise}
			\end{cases}$
			\If{$\tilde{y_t} = y_t$}
				\State $w_{t + 1, i} \gets w_{t, i}, \forall i \in [n]$
			\Else
			\State $w_{t + 1, i} \gets \begin{cases}
				\beta \cdot w_{t, i}, & \tilde{y_{t, i}} \neq y_t\\
				w_{t, i}, & \tilde{y_{t, i}} = y_t
			\end{cases}, \forall i \in [n]$
			\EndIf

		\EndFor
	\end{algorithmic}
\end{algorithm}

即每轮选择 $\tilde{y_t}$ 为 $n$ 位专家预测的加权  marjority, 如果出错了, 就把所有导致自己出错的专家的权值乘上 $\beta$ 作为惩罚.

\begin{theorem}
	记 $L_T = \sum\limits_{t=1}^{T}\mathbbm 1[\tilde{y_t} \neq y_t]$ 为学习者的 loss, $m_T^* = \min\limits_{i \in [n]}\sum\limits_{t=1}^{T}\mathbbm 1[\tilde{y_{t, i}} \neq y_t]$ 为最好的专家的 loss, 则在 Weighted Majority Vote 算法下, 有 $$L _T \le \frac{m_T^*\log(1 / \beta) + \log n}{\log(2/(1 + \beta))}$$
\end{theorem}
\begin{proof}
	注意到 \num{1} $T$ 轮结束后, 所有专家剩余的总权值至少还有 $\beta^{m_T^*}$, \num{2} 每次学习者出错都会导致总权值乘上不大于 $\frac{1 + \beta}{2}$ 的系数, 故
	$$\beta^{m_T^*} \le n\left(\frac{1 + \beta}{2}\right)^{L_T} \Rightarrow L_T \le \frac{m_T^*\log(1 / \beta) + \log n}{\log(2/(1 + \beta))}$$
\end{proof}
\begin{remark}
	考虑 $\beta \to 1$, 由 L'Hospital Rule 可知 $\frac{\log(1 / \beta)}{\log(2 / (1 + \beta))} \to 2$, 即 Weighted Majority Vote 算法给出的最好的界中, $m_T^*$ 前的系数至少是 $2$. 接下来的 Randomized Weighted Updating 算法会给出更好的界.
\end{remark}
\subsubsection{Randomized Weighted Updating}
\begin{algorithm}
	\caption{Randomized Weighted Updating}
	\begin{algorithmic}[1]
		\State Initialize $w_{1, i} \gets 1, \forall i \in [n]$
		\State Choose parameter $\beta \in [\frac12, 1)$
		\For{$t = 1 \to T$}
			\State Chooes $\tilde{y_t} = \tilde{y_{t, i}}$ with probability proportional to $w_{t, i}$
			\State $w_{t + 1, i} \gets \begin{cases}
				\beta \cdot w_{t, i}, & \tilde{y_{t, i}} \neq y_t\\
				w_{t, i}, & \tilde{y_{t, i}} = y_t
			\end{cases}, \forall i \in [n]$
		\EndFor
	\end{algorithmic}
\end{algorithm}
\begin{theorem}
	在Randomized Weighted Updating 算法下, 有 $$\E{L_T} \le (2 - \beta)m_T^* + \frac{\ln n}{1 - \beta}$$
	\label{random-weighted-update}
\end{theorem}
\begin{proof}
	注意到权值的更新无关于每轮有没有答错, 因此 $\mathbbm 1 [\tilde{y_t} \neq y_t]$ 是独立随机变量. 
	
	第 $i$ 轮结束后, 总权值的变化一定是 $W \to W(1 - (1 - \beta)\P{\tilde{y_t} \neq y_t})$, 由于 $\E{L_T} = \sum\limits_{t=1}^T\P{\tilde{y_t} \neq y_t}$, 因此

	$$\beta^{m_T^*} \le n \prod_{t=1}^{T}(1 - (1 - \beta)\P{\tilde{y_t} \neq y_t}) \le n\prod_{t=1}^{T}\e^{-(1-\beta)\P{\tilde{y_t} \neq y_t}} = n\e^{-(1 - \beta)\E{L_T}}$$ 从而得到了 $$\E{L_T} \le \frac{\ln(1/\beta)m_T^* + \ln n}{1 - \beta}$$

	只需要进一步证明 $\frac{\ln(1 / \beta)}{1 - \beta} \le 2 - \beta$. 考虑函数 $f(\beta) = \ln\beta + (1 - \beta)(2 - \beta), f'(\beta) = \frac{(1-  \beta)(1 - 2\beta)}{\beta}$, 当 $\beta \in [\frac12, 1)$ 时恒有 $f'(\beta) \le 0$, 从而 $f(\beta) \ge f(1) = 0$, 说明了 $\ln(1 / \beta) \le (1 - \beta)(2 - \beta), \frac{\ln(1 / \beta)}{1 - \beta} \le 2 - \beta$.
\end{proof}
\subsubsection{Hedge Algorithm}
我们再提出一种叫做 Hedge Algorithm 的算法, 它其实只是 Randomized Weighted Updating 的推广, 但这个结果可以为后续证明\cref{minimax} 的工作做准备.

在 Hedge Algorithm 的设定下, loss 不再是“答错了几次”, 而是每一轮每一位专家的回答都有一个loss $g_t(i) \in [0, 1]$, 记学习者在第 $t$ 轮的 loss 为 $l_t$, 则 $l_t$ 的期望就是 $n$ 位专家的加权平均: $$\E{l_t} = \left(\sum\limits_{i=1}^{n}w_{t, i}g_t(i)\right) \bigg/ \left(\sum\limits_{i=1}^{n}w_{t, i}\right)$$

\begin{algorithm}
	\caption{Hedge Algorithm}
	\begin{algorithmic}[1]
		\State Initialize $w_{1, i} \gets 1, \forall i \in [n]$
		\State Choose parameter $\beta \in (0, 1)$
		\For{$t = 1 \to T$}
			\State Chooes $i_t \in [n]$ with probability proportional to $w_{t, i}$, and obtain the loss $l_t = g_t(i_t)$
			\State $w_{t + 1, i} \gets w_{t, i} \cdot \beta^{g_t(i)}, \forall i \in [n]$
		\EndFor
	\end{algorithmic}
\end{algorithm}

\begin{theorem}
	重新定义 $L_T = \sum\limits_{t=1}^{T}l_t$, 在 Hedge Algorithm 下, 有
	$$\E{L_T} - \min_{i \in [n]}\sum_{t=1}^{T}g_t(i) = O(\sqrt{T\log n})$$
	\label{Hedge}
\end{theorem}
\begin{proof}
	仍然注意到 $l_t$ 是独立随机变量.

	第 $i$ 轮结束后, 总权值的变化是 $W \to W \cdot \E{\beta^{l_t}}$, 从而有
	\begin{equation*}
		\begin{split}
			\e^{-\ln(1/\beta)m_T^*} = \beta^{m_T^*} &\le n\prod_{t=1}^{T}\E{\beta^{l_t}} = n\prod_{t=1}^{T}\E{\e^{-\ln(1/\beta)l_t}} \\
			&\le n\prod_{t=1}^{T}\E{1 - \ln(1/\beta)l_t + \ln^2(1/\beta)l_t^2} \\
			&\le n\prod_{t=1}^{T}\left(1 - \ln(1/\beta)\E{l_t} + \ln^2(1/\beta)\right) \\
			&\le n\prod_{t=1}^{T}\e^{-\ln(1/\beta)\E{l_t} + \ln^2(1/\beta)} \\
			&= n\e^{-\ln(1/\beta)\E{L_T} + T\ln^2(1/\beta)}
		\end{split}
	\end{equation*}
	其中 $m_T^* = \min_{i \in [n]}\sum_{t=1}^{T}g_t(i)$. 两边取对数得到

	$$\E{L_T} - \min_{i \in [n]}\sum_{t=1}^{T}g_t(i) \le \frac{\ln n}{\ln(1/\beta)} + T\ln(1/\beta) \le 2\sqrt{T\ln n} = O(\sqrt{T\log n})$$
\end{proof}

\subsection{Proof of Minimax Theorem via Online Learning}

在 Game Theory 一章中, 我们陈述了 Minimax Theorem (\cref{minimax}), 其表明在混合策略的双人零和博弈下, 先后手并不会影响博弈的最终结果. 接下来我们利用在线学习的技术来证明这个结论.

$$\min_p\max_q p^{\text T}Mq = \max_q\min_p p^{\text T}Mq$$

\subsubsection{The $\ge$ Direction}

这个方向的结论应该是平凡的, 直观上来说就是“后手总不劣于先手”. 

形式化地, 记 $p^* = \arg\min_p\max_qp^{\text T}Mq$ 为 row player 后手时选择的最优的 $p$, $q^* = \arg\max_q\min_pp^{\text T}Mq$ 为 column player 后手时选择的最优的 $q$, 则 $$\min_p\max_q p^{\text T}Mq = \max_q {p^*}^{\text T}Mq \ge {p^*}^{\text T}Mq^* \ge \min_p p^{\text T}Mq^* = \max_q\min_p p^{\text T}Mq$$

\subsubsection{The $\le$ Direction}

row player 对应在线学习中的学习者, column player 对应 adversary, 收益矩阵 $M$ 的 $m$ 行分别是一位专家.

在第 $t$ 轮中, 学习者选择列向量 $p_t$ 满足 $(p_t)_i = \frac{w_{t, i}}{\sum_{i=1}^{m}w_{t, i}}$, 其中 $w_{t, i}$ 表示第 $t$ 轮时第 $i$ 位专家的权值. 给出了 $p_t$ 后, adversary 可以很容易地给出 $q_t = \max_{q}p_t^{\text T}Mq$. 第 $i$ 位专家建议选第 $i$ 行, 他这样的方案对应的 loss 是 $g_t(i) = (Mq_t)_i$. 显然学习者此时的 loss 的期望恰好等于 $m$ 为专家各自损失的加权平均, 即 $$\E{l_t} = \left(\sum\limits_{i=1}^{n}w_{t, i}g_t(i)\right) \bigg/ \left(\sum\limits_{i=1}^{n}w_{t, i}\right) = p_t^{\text T}Mq_t$$

由 Hedge Algorithm 以及\cref{Hedge}, 我们知道了

$$\E{L_T} - \min_{i \in [n]}\sum_{t=1}^{T}g_t(i) = \sum_{t=1}^{T}p_t^{\text T}Mq_t - \min_{i \in [n]}\left(M\sum_{t=1}^{T}q_t\right)_i \le O(\sqrt{T\log m})$$

由此得到
\begin{equation*}
	\begin{split}
		\frac1T\sum_{t=1}^{T}p_t^{\text T}Mq_t &\le \min_{i \in [n]}\left(M\sum_{t=1}^{T}q_t\right)_i + O\left(\sqrt{\frac{\log m}{T}}\right)\\
		&= \min_p\left(p^{\text T}M\left(\frac1T\sum_{t=1}^Tq_t\right)\right) + o(1)\\
		&\le \max_q\min_p p^{\text T}Mq + o(1)
	\end{split}
\end{equation*}
(其中$O\left(\sqrt{\frac{\log m}{T}}\right) = o(1)$ 因为我们视 $m$ 为常数)而又注意到 $$\min_p\max_q p^{\text T}Mq \le \max_q \left(\frac1T\sum_{t=1}^{T}p_t^{\text T}\right)Mq \le \frac1T\sum_{t=1}^T\max_qp_t^{\text T}Mq = \frac1T\sum_{t=1}^{T}p_t^{\text T}Mq_t$$

因此 $\min_p\max_q p^{\text T}Mq \le \max_q\min_p p^{\text T}Mq + o(1)$, 即 $\min_p\max_q p^{\text T}Mq \le \max_q\min_p p^{\text T}Mq$.

\subsection{Multi-arm Bandits (MAB) Problem}
有一台多臂老虎机, 它有 $k$ 个拉杆, 第 $i$ 个拉杆拉动后会返回一个服从分布 $\mathcal D_i$ 的随机变量loss, 其中 $\mathcal D_i$ 的均值为 $\mu_i$. 需要最小化拉 $T$ 轮后得到的 loss 之和.

记第 $t$ 轮中选择拉动了第 $a_t$ 个拉杆, 我们可以定义 $T$ 轮操作后的 regret 为 $$R_T = \mathbb E_{\mathcal A}\left[\sum_{t=1}^T\mu_{a_t} - \mu^*\right]$$ 其中 $\mathcal A = (a_1, \cdots, a_T)$ 为选择拉动的拉杆编号序列, $\mu^* = \min_{1 \le i \le k}\mu_i$ 为最优的期望 loss. 概率源自于 $\mathcal A$ 中 $a_t$ 的选取会基于之前随机变量 $l_1 \sim \mathcal D_{a_1}, \cdots, l_{t-1} \sim \mathcal D_{t-1}$ 的实际取值.

由于在 MAB 问题中我们并不先验地知道每个分布的均值, 因此这是一个在 exploration (调查每个拉杆) 和 exploitation (对着“最好”的薅) 之间权衡的过程.

\subsubsection{UCB Algorithm}

\begin{algorithm}
	\caption{UCB Algorithm}
	\begin{algorithmic}[1]
		\State $n_t(a)$ represents \# of times arm $a$ has been pulled at time $t$
		\State $\mu_t(a)$ represents the empirical loss of arm $i$ at time $t$
		\State Initialize $n_0(a) \gets 0, \mu_0(a) \gets 0$
		\For{$t = 1 \to T$}
			\State For each arm $a$, compute $\text{UCB}_t(a) \gets \mu_{t-1}(a) - \sqrt{\frac{\ln T}{n_{t-1}(a)}}$
			\State Pull the arm $a_t = \arg \min_{1 \le a \le k}\text{UCB}_t(a)$
			\State Update $n_t(a)$ and $\mu_t(a)$ for each $1 \le a \le k$
		\EndFor
	\end{algorithmic}
\end{algorithm}

特别地, 当 $n_{t-1}(a) = 0$ 时, 记 $\text{UCB}_t(a) = -\infty$.

\begin{theorem}
	不失一般性假设 $\mu_1 \le \min\{\mu_2, \cdots, \mu_k\}$, UCB Algorithm 得到的 regret 可以被限制为
	$$R_T = \mathbb E_{\mathcal A}\left[\sum_{t=1}^T\mu_{a_t} - \mu_1\right] \le \sum_{\Delta_a > 0}\left(\frac{16\ln T}{\Delta_a} + 2\Delta_a\right)$$
	其中 $\Delta_a = \mu_a - \mu_1$.
\end{theorem}
\begin{proof}
	注意到 $$R_T = \mathbb E_{\mathcal A}\left[\sum_{t=1}^T\mu_{a_t} - \mu_1\right] = \sum_{a=1}^{k}\Delta_a \cdot \mathbb E_{\mathcal A}[n_T(a)]$$ 只需要证明对于每个 $\Delta_a > 0$ 的拉杆 $a$, 都有 $$\mathbb E_{\mathcal A}[n_T(a)] \le \frac{16\ln T}{\Delta_a^2} + 2$$

	对于任意正整数 $m$, 我们有 \begin{equation*}
		\begin{split}
			\mathbb E_{\mathcal A}[n_T(a)] &= \sum_{t=1}^T \P{a_t = a} \\
			&= \sum_{t=1}^T \P{a_t = a \wedge n_{t-1}(a) < m} + \sum_{t=1}^T \P{a_t = a \wedge n_{t-1}(a) \ge m}\\
			&\le m + \sum_{t=m+1}^T \P{a_t = a \wedge n_{t-1}(a) \ge m}
		\end{split}
	\end{equation*}

	\begin{proposition}
		假如某个 $\Delta_a > 0$ 的拉杆 $a$ 在第 $t$ 轮被拉动了, 则要么 $\text{UCB}_t(1) > \mu_1$, 要么 $\text{UCB}_t(a) < \mu_1 = \mu_a - \Delta_a$.
	\end{proposition}
	\begin{proof}
		否则 $\text{UCB}_t(1) \le \mu_1 \le \text{UCB}_t(a)$, 拉动 $a$ 不如拉动 $1$.
	\end{proof}

	利用上述命题, \begin{equation*}
		\begin{split}
			\P{a_t = a \wedge n_{t-1}(a) \ge m} &= \P{\left(\text{UCB}_t(1) > \mu_1 \vee \text{UCB}_t(a) < \mu_1\right) \wedge n_{t-1}(a) \ge m}\\
			&\le \P{\text{UCB}_t(1) > \mu_1 \wedge n_{t-1}(a) \ge m} + \P{\text{UCB}_t(a) < \mu_1 \wedge n_{t-1}(a) \ge m}\\
			&\le \P{\text{UCB}_t(1) > \mu_1} + \P{\text{UCB}_t(a) < \mu_1 \wedge n_{t-1}(a) \ge m}
		\end{split}
	\end{equation*}

	对于前一部分, \begin{equation*}
		\begin{split}
			\P{\text{UCB}_t(1) > \mu_1} &= \sum_{k=1}^T \P{\mu_{t-1}(1) - \sqrt{\frac{\ln T}{k}} > \mu_1 \wedge n_{t-1}(1) = k} \\
			&\le \sum_{k=1}^T \P{\mu_{t-1}(1) - \mu_1 > \sqrt{\frac{\ln T}{k}} \bigg| n_{t-1}(1) = k}\\
			&\le \sum_{k=1}^T \exp\left(-2k\left(\sqrt{\frac{\ln T}{k}}\right)^2\right) \\
			&= \sum_{k=1}^T \frac{1}{T^2} = \frac 1T
		\end{split}
	\end{equation*}
	其中第三行用到了 Chernoff Bound (\cref{chernoff-bound-additive}).
	
	对于第二个, 取 $m$ 满足 $2\sqrt{\frac{\ln T}{m}}\le \Delta_a \le 4\sqrt{\frac{\ln T}{m}}$, 有 $m \le \frac{16 \ln T}{\Delta_a^2}$, \begin{equation*}
		\begin{split}
			\P{\text{UCB}_t(a) < \mu_1 \wedge n_{t-1}(a) \ge m} &= \P{\mu_{t-1}(a) - \mu_a < \sqrt{\frac{\ln T}{n_{t-1}(a)}} - \Delta_a \wedge n_{t-1}(a) \ge m}\\
			&\le \P{\mu_{t-1}(a) - \mu_a < -\sqrt{\frac{\ln T}{m}}} \le \frac1T
		\end{split}
	\end{equation*}

	结合上述结果, 我们得到
	$$\mathbb E_{\mathcal A}[n_T(a)] \le m + \sum_{t=m+1}^T \P{a_t = a \wedge n_{t-1}(a) \ge m} \le m + \sum_{t=m+1}^T \frac2T \le \frac{16\ln T}{\Delta_a^2} + 2$$

	完成了证明.
\end{proof}
\begin{remark}
	上述结论是 instance-dependent 的, 因为其限制中包含了 $\Delta_a$ 项. 如果我们把 $\Delta_a$ 视作常数, 那么 $R_T$ 就是 $O(k\ln T)$ 级别, 这比 Online Learning with Expert Advice 的 $O(\sqrt{T\log n})$ 要厉害. 不过, 如果 $\Delta_a$ 很小, 上述结论给出的界就会很差. 接下来我们给出一个更精细化的结论.
\end{remark}
\begin{theorem}
	假设 $\Delta_a$ 有界(存在 $M > 0$ 使得 $\Delta_a \le M$ 成立), 则最坏情况下 UCB Algorithm 得到的 regret 为 $$R_T = O(\sqrt{kT\ln T})$$
\end{theorem}
\begin{proof}
	取 $\delta = \sqrt{\frac{k\ln T}{T}}$, 将所有 $a$ 按照 $\Delta_a$ 与 $\delta$ 的大小关系分为两组: \begin{equation*}
		\begin{split}
			R_T^{(1)} &= \sum_{t=1}^{T} \sum_{0 < \Delta_a < \delta} \Delta_a \cdot \P{a_t = a} \le T \cdot \delta \le \sqrt{kT\ln T}\\
			R_T^{(2)} &= \sum_{\Delta_a \ge \delta}\left(\frac{16\ln T}{\Delta_a} + 2\Delta_a\right) = O\left(\frac{kT}{\delta} + k\right) = O(\sqrt{kT\ln T})\\
			R_T &= R_T^{(1)} + R_T^{(2)} = O(\sqrt{kT\ln T})
		\end{split}
	\end{equation*}
\end{proof}

\subsubsection{Thompson Sampling}
\begin{definition}[$Beta$ 分布]
	$Beta$ 分布是在区间 $(0, 1)$ 上的连续分布, 对于参数 $\alpha, \beta > 0$, 分布 $Beta(\alpha, \beta)$ 的概率密度函数为 $$f(x; \alpha, \beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1}$$
\end{definition}

在 Thompson Sampling 中, 我们假设所有 loss 都是 $[0, 1]$ 的.

\begin{algorithm}
	\caption{Thompson Sampling}
	\begin{algorithmic}[1]
		\State Initialize $S_a \gets 0, F_a \gets 0$
		\For{$t = 1 \to T$}
			\State For each arm $a$, sample $\Theta_a(t) \sim Beta(S_a + 1, F_a + 1)$
			\State Pull the arm $a_t = \arg \max_{1 \le a \le k}\Theta_a(t)$, and obtain the loss $l_t \sim \mathcal D_{a_t}$ \Comment{$l_t \in [0, 1]$}
			\State Sample $\tilde{l_t} \sim \mathcal B(1, l_t)$ \Comment{$\tilde{l_t} \in \{0, 1\}$}
			\State $S_{a_t} \gets S_{a_t} + \tilde{l_t}, F_{a_t} \gets F_{a_t} + 1 - \tilde{l_t}$
		\EndFor
	\end{algorithmic}
\end{algorithm}
\begin{theorem}
	不失一般性假设 $\mu_1 \le \min\{\mu_2, \cdots, \mu_k\}$, 对于任意 $\varepsilon > 0$, Thompson Sampling 得到的 regret 都满足 $$R_T \le (1 + \varepsilon)\sum_{\mu_a \neq \mu_1}\frac{\Delta_a\log T}{D(\mu_a \| \mu_1)} + O\left(\frac{k}{\varepsilon^2}\right) \le (1 + \varepsilon)\sum_{\mu_a \neq \mu_1}\frac{\log T}{2\Delta_a} + O\left(\frac{k}{\varepsilon^2}\right)$$
\end{theorem}

证明太长了, 就不讲了.

\section{Differential Privacy}

设计差分隐私的主要目的, 是在不透露过多个体信息(privacy)的前提下, 提供尽量多, 或者尽量准确的整体统计信息(non-privacy). 有一种简单的想法, 就是给输出的整体信息加 noise.

\begin{definition}[相邻数据集]
	考虑 $D = \{x_1, x_2, \cdots, x_n\}, x_i \in \mathcal X$ 是单个数据点, 则 $D \in \mathcal X^n$ 是大小为 $n$ 的数据集. 两个数据集 $D, D'$ 被称为相邻的, 如果其只存在一位不同.
\end{definition}
\begin{definition}[统计查询]
	一个依据 $h: \mathcal X \to \{0, 1\}$ 定义的统计查询 $Q$ 是 $\mathcal X^n \to \mathbb Q$ 的映射, 满足 $$Q(D) = \frac{1}{|D|} \sum_i h(x_i)$$
\end{definition}
\begin{definition}[差分隐私]
	令 $A$ 为一个在输入数据集 $D$ 上运行的随机算法, 称 $A$ 满足 $\varepsilon$-差分隐私, 如果对于任意相邻数据集 $D, D' \in \mathcal X^n$, 任意 $S \subseteq \im A$, 都有 $$\P{A(D) \in S} \le \e^{\varepsilon}\P{A(D') \in S}$$
\end{definition}
\begin{definition}[$(\alpha, \beta)$-精确]
	称随机算法 $A$ 对于统计查询 $Q$ 满足 $(\alpha, \beta)$-精确, 如果对于任意 $D \in \mathcal X^n$, $$\P{|A(D) - Q(D)| \ge \alpha} \le \beta$$
\end{definition}
\subsection{Laplace Mechanism}
\begin{definition}[拉普拉斯分布]
	随机变量 $X$ 服从参数为 $\mu, \sigma$ 的拉普拉斯分布(记作 $X \sim \text{Lap}(\mu, \sigma)$), 其密度函数为 $$f_X(x) = \frac{1}{2\sigma}\exp\left(\frac{|x-\mu|}{\sigma}\right)$$
\end{definition}
\begin{definition}[Laplace Mechanism, or Additive noise mechanism]
	Laplace Mechanism 就是在一个统计查询 $Q: \mathcal X^n \to \mathbb Q$ 的基础上, 添加一个服从分布 $\text{Lap}(\mu = 0, \sigma)$ 的noise, 得到 $A: \mathcal X^n \to \mathbb R$, $A(D) = Q(D) + Z, Z \sim \text{Lap}(0, \sigma)$.
\end{definition}
\begin{theorem}
	Laplace Mechanism 满足 $\varepsilon$-差分隐私以及 $(\alpha, \beta)$-精确, 其中(假设 $\beta$ 是常数) $\varepsilon = \frac{1}{n\sigma}, \alpha = \sigma \ln \frac{1}{\beta}$.
\end{theorem}
\begin{proof}
	对于任意 $a \in \im A = \mathbb R$, 有 $$\frac{\P{A(D) = a}}{\P{A(D') = a}} = \frac{f_Z(a - Q(D))}{f_Z(a - Q(D'))} = \frac{\frac{1}{2\sigma}\exp\left(-\frac{a-Q(D)}{\sigma}\right)}{\frac{1}{2\sigma}\exp\left(-\frac{a-Q(D')}{\sigma}\right)} \le \exp\left(\frac{|Q(D) - Q(D')|}{\sigma}\right) \le \exp\left(\frac{1}{n\sigma}\right)$$ \footnote{这里 $A(D) = a$ 实际上想表达的是 $a \le A(D) < a + \text dt$.}故 $\varepsilon = \frac{1}{n \sigma}$. 至于 $\alpha$, 
	$$\P{|A(D) - Q(D)| \ge \alpha} = \int_{-\infty}^{-a}\frac{1}{2\sigma}\exp\left(-\frac t{\sigma}\right)\text dt + \int_a^{\infty}\frac{1}{2\sigma}\exp\left(-\frac t{\sigma}\right)\text dt = \exp\left(-\frac{\alpha}{\sigma}\right) = \beta \Rightarrow \alpha = \sigma \ln \frac{1}{\beta}$$
\end{proof}
\begin{definition}[多组查询下的精确]
	记 $Q = (Q_1, \cdots, Q_k): \mathcal X^n \to \mathbb Q^k$ 是 $k$ 次统计查询, 令 $A = (A_1, \cdots, A_k)$ 为针对每个查询, 用 Laplace Mechanism 构造的随机算法, $A_i(D) - Q_i(D) \sim \text{i.i.d. } \text{Lap}(0, \sigma)$. 称 $A$ 对于 $Q$ 满足 $(\alpha, \beta)$-精确, 如果对于任意 $D \in \mathcal X^n$, $$\P{\|A(D) - Q(D)\|_{\infty} \ge \alpha} \le \beta$$
\end{definition}
\begin{theorem}
	当每个 Laplace Mechanism $A_i$ 都满足 $\varepsilon$-差分隐私和 $(\alpha, \beta)$-精确时, $A = (A_1, \cdots, A_k)$ 满足 $k\varepsilon$-差分隐私和 $(\alpha, k\beta)$-精确.
\end{theorem}
\begin{proof}
	\begin{itemize}
		\item $k\varepsilon$-差分隐私: 注意到 noise 是 i.i.d. 的, 故联合分布密度等于各自相乘, 由 $f_{A_i(D)}(x_i) \le \e^{\varepsilon}f_{A_i(D')}(x_i)$ 可以很容易得到 $f_{A_i(D)}(\vec x) \le \e^{k\varepsilon} f_{A_i(D')}(\vec x)$.
	    \item $(\alpha, k\beta)$-精确: 使用 Union Bound 即可.
	\end{itemize}
\end{proof}
\begin{corollary}
	对于多组查询 $Q = (Q_1, \cdots, Q_k)$, Laplace Mechanism 满足 $\frac{k}{n\sigma}$-差分隐私以及 $(\sigma\ln\frac{k}{\beta}, \beta)$-精确.
\end{corollary}

注意到在多组查询的 Laplace Mechanism 下, 如果要求 $\varepsilon = O(1), \alpha = o(1)$, $\beta$ 是常数, 则不得不有 $k = o\left(\frac{n}{\ln n}\right)$. 换句话说, 隐私泄露 $\varepsilon$ 是关于查询次数 $k$ 线性的, 从某种程度上说, 这是不能接受的.

接下来我们将介绍一种 highly-nontrivial 的机制设计, 使得可以在保证隐私与精确的前提下做到 $k >> n$ 的查询次数.

\subsection{BLR Mechanism}

不妨假设样本空间是有限大的, 即 $|\mathcal X| = N$. 此外沿用之前的一些记号, $k$ 表示查询次数, $n = |D|$ 表示单个数据集大小, $\varepsilon$ 表示隐私性, $(\alpha, \beta)$ 表示精确性. 额外记 $\sigma = \frac{2}{n\varepsilon}$.

该机制采用的随机算法 $\mathcal A$ 不再返回一个 $\mathbb R^d$ 向量, 取而代之的是返回 $\mathcal X^m$ 即 $m$ 个样本, 其中 $m = \frac{2\log(2k)}{\alpha^2}$. 对于 $D \in \mathcal X^n$, $\mathcal A(D)$ 返回 $\hat{D} \in \mathcal X^m$ 的概率 $\P{\mathcal A(D) = \hat{D}}$ 正比于 $\exp\left(\frac{u(D, \hat{D})}{\sigma}\right)$, 其中 $u$ 是\obj{效用函数(utility function)}, 用于衡量「在输入 $D$ 时返回 $\hat{D}$ 有多好」, 在这里定义为 $$u(D, \hat{D}) = -\max_{i=1}^{k}|Q_i(D) - Q_i(\hat{D})|$$ 其中 $Q_i$ 表示第 $i$ 个统计查询.

\begin{theorem}
	BLR Mechanism 满足 $\varepsilon$-差分隐私以及 $(\alpha, \beta)$-精确, 其中 $$\alpha = O \left(\left(\frac{\log k \log N + \log(1 / \beta)}{n\varepsilon}\right)^{1/3}\right)$$
\end{theorem}
\begin{proof}
	记 $\Delta u = \max_{D, D', \hat{D}}|u(D, \hat{D}) - u(D', \hat{D})|$, 可以观察到 $\Delta u \le \frac1n$.

	考虑 \begin{align*}
		\begin{split}
			\P{\mathcal A(D) = \hat{D}} &= \frac{\exp\left(\frac{u(D, \hat{D})}{\sigma}\right)}{\sum_{\hat{D}}\exp\left(\frac{u(D, \hat{D})}{\sigma}\right)}\\
			\P{\mathcal A(D') = \hat{D}} &= \frac{\exp\left(\frac{u(D', \hat{D})}{\sigma}\right)}{\sum_{\hat{D}}\exp\left(\frac{u(D', \hat{D})}{\sigma}\right)}
		\end{split}
	\end{align*}
	从而得到 \begin{align*}
		\begin{split}
			\frac{\P{\mathcal A(D) = \hat{D}}}{\P{\mathcal A(D') = \hat{D}}} &\le \frac{\sum_{\hat{D}}\exp\left(\frac{u(D', \hat{D})}{\sigma}\right)}{\sum_{\hat{D}}\exp\left(\frac{u(D, \hat{D})}{\sigma}\right)}\exp\left(\frac{u(D, \hat{D}) - u(D', \hat{D})}{\sigma}\right) \\
			&\le \frac{\sum_{\hat{D}}\exp\left(\frac{\Delta u}{\sigma}\right)\exp\left(\frac{u(D, \hat{D})}{\sigma}\right)}{\sum_{\hat{D}}\exp\left(\frac{u(D, \hat{D})}{\sigma}\right)}\exp\left(\frac{\Delta u}{\sigma}\right) \\
			&\le \exp\left(\frac{2\Delta u}{\sigma}\right) \le \e^{\varepsilon}
		\end{split}
	\end{align*}
	

	至于 $(\alpha, \beta)$-精确, 事实上在 $\mathcal A$ 的返回值不是实数后我们还没有定义 $(\alpha, \beta)$-精确到底是什么, 所以在这里重新定义一下: 称 $\mathcal A$ 满足 $(\alpha, \beta)$-精确, 如果 $$\P{u(D, \mathcal A(D)) \le u^* - \alpha} \le \beta$$

	其中 $u^* = \max_{D, \hat{D}}u(D, \hat{D})$. 在 BLR Mechanism 中有 $u^* = 0$.

	考虑 $\mathcal A$ 的像空间$\mathcal X^m$ 中的一个元素 $\hat{D}$, 称 $\hat{D} \in G$ 如果 $u(D, \hat{D}) \ge u^* - \frac{\alpha}2$, 称 $\hat{D} \in B$ 如果 $u(D, \hat{D}) \le u^* - \alpha$. $G$ 和 $B$ 分别表示 good 和 bad.

	\begin{lemma}
		如果能证明 $G \neq \varnothing$, 则
	\end{lemma}
	\begin{proof}
		\begin{align*}
			\begin{split}
				\P{u(D, \mathcal A(D)) \le u^* - \alpha} &= \P{\mathcal A(D) \in B} \le \frac{\P{\mathcal A(D) \in B}}{\P{\mathcal A(D) \in G}} \le \frac{\exp\left(\frac{u^* - \alpha}{\sigma}\right)\cdot |B|}{\exp\left(\frac{u^* - \frac{\alpha}{2}}{\sigma}\right)\cdot |G|} \\&\le \exp\left(-\frac{\alpha}{2\sigma}\right)|\mathcal X|^m = \exp\left(-\frac{\alpha n \varepsilon}{4}\right)|\mathcal X|^m \le \beta
			\end{split}
		\end{align*}
	\end{proof}
	
	剩下的开摆了.
\end{proof}

\newpage
\section{Reinforcement Learning}
定义一些记号
\begin{itemize}
	\item $\mathcal S$: state space
    \item $\mathcal A$: actione space
    \item $S_t \in \mathcal S, A_t \in \mathcal A, R_t \in \mathbb R$: state, action, and rewrad at time $t$
    \item $P_{s, s'}^{a} = \mathbb P(S_{t+1} = s' | S_t = s, A_t = a)$: transition probability
    \item $R(s, a) = \mathbb E[R_{t+1} | S_t = s, A_t = a]$: reward function
    \item $\gamma \in (0, 1)$: discount factor
    \item $\pi: \mathcal S \to \Delta(\mathcal A)$: policy, where $\Delta(\mathcal A)$ denotes the space of probability distribution over $\mathcal A$
    \item $v^{\pi}(s) = \mathbb E\left[\sum\limits_{k \ge 0}\gamma^kR_{t+k} \bigg| S_t = s\right]$: (state) value function, where probability is over \num{i} the policy $\pi$, \num{2} the transition probability $P_{s, s'}^a$.
\end{itemize}
\begin{proposition}[Bellman Expectation Equation]
	$$v^{\pi}(s) = \mathbb E_{a \sim \pi(s)}\left[R(s, a) + \gamma\sum_{s' \in \mathcal S}P_{s, s'}^av^{\pi}(s')\right]$$
\end{proposition}
考虑在固定 policy $\pi$ 下的 Bellman Expectation Operator $\Phi: \mathbb R^{N} \to \mathbb R^{N}$, 满足 $$\Phi(\mathbf v)(s) = \mathbb E_{a \sim \pi(s)}\left[R(s, a) + \gamma\sum_{s' \in \mathcal S}P_{s, s'}^a\mathbf v(s')\right]$$
则可以证明 $\Phi$ 是一个无穷范数下的 $\gamma$-压缩映射(contraction mapping).

这说明随便找一个 $\mathbf v_0 \in \mathbb R^N$, 不断对它做 $\Phi$ 这个 operator, 它都会收敛到唯一的不动点, 这个点就是 $\pi$ 的 value function $v^{\pi}$.

\subsection{Finding Optimal Policy}

\begin{definition}[Bellman Operator]
	无关 policy $\pi$, 定义 Bellman Operator $\Phi^*: \mathbb R^{N} \to \mathbb R^{N}$, 满足 $$\Phi(\mathbf v)(s) = \max_{a \in \mathcal A}\left[R(s, a) + \gamma\sum_{s' \in \mathcal S}P_{s, s'}^a\mathbf v(s')\right]$$
\end{definition}
\begin{theorem}
	Bellman Operator $\Phi^*$ 是无穷范数下的 $\gamma$-压缩映射.
\end{theorem}
\begin{proof}
	考虑 $\mathbf u, \mathbf v \in \mathbb R^N$, 记 \begin{align*}
		a_{\mathbf u} &= \arg\max_a\left[R(s, a) + \gamma\sum_{s' \in \mathcal S}P_{s, s'}^a\mathbf u(s')\right] \\
		a_{\mathbf v} &= \arg\max_a\left[R(s, a) + \gamma\sum_{s' \in \mathcal S}P_{s, s'}^a\mathbf v(s')\right] \\
	\end{align*}
	此时有(不妨设$\Phi^*(\mathbf v)(s) \ge \Phi^*(\mathbf u)(s)$) \begin{align*}
		\Phi^*(\mathbf v)(s) - \Phi^*(\mathbf u)(s) &= \left[R(s, a_{\mathbf v}) + \gamma\sum_{s' \in \mathcal S}P_{s, s'}^{a_{\mathbf v}}\mathbf v(s')\right] - \left[R(s, a_{\mathbf u}) + \gamma\sum_{s' \in \mathcal S}P_{s, s'}^{a_{\mathbf u}}\mathbf u(s')\right] \\
		&\le \left[R(s, a_{\mathbf v}) + \gamma\sum_{s' \in \mathcal S}P_{s, s'}^{a_{\mathbf v}}\mathbf v(s')\right] - \left[R(s, a_{\mathbf v}) + \gamma\sum_{s' \in \mathcal S}P_{s, s'}^{a_{\mathbf v}}\mathbf u(s')\right] \\
		&= \gamma\sum_{s' \in \mathcal S}P_{s, s'}^{a_{\mathbf v}}(\mathbf v(s') - \mathbf u(s')) \\
		&\le \gamma\sum_{s' \in \mathcal S}P_{s, s'}^{a_{\mathbf v}}\|\mathbf v - \mathbf u\|_{\infty} \\
		&= \gamma \|\mathbf v - \mathbf u\|_{\infty}
	\end{align*}
	从而也有 $\|\Phi^*(\mathbf v) - \Phi^*(\mathbf u)\|_{\infty} \le \gamma \|\mathbf v - \mathbf u\|_{\infty}$
\end{proof}
\begin{theorem}
	对于任意的 policy $\pi_0$, 记 $v^{\pi_0}$ 为其 value function, 则 $\Phi^*(v^{\pi_0})(s) \ge v^{\pi_0}(s)$ 对任意 $s \in \mathcal S$ 成立.
\end{theorem}
\begin{proof}
	\begin{align*}
		v^{\pi_0}(s) &= \mathbb E_{a \sim \pi_0(s)}\left[R(s, a) + \gamma\sum_{s' \in \mathcal S}P_{s, s'}^av^{\pi_0}(s')\right] \\
		\Phi^*(v^{\pi_0})(s) &= \max_{a \in \mathcal A}\left[R(s, a) + \gamma\sum_{s' \in \mathcal S}P_{s, s'}^av^{\pi_0}(s')\right]
	\end{align*}
	一目了然了属于是.
\end{proof}

现在有一个问题: 我们有一个 policy $\pi_0$, 我们可以对 $\pi_0$ 的 value function $v^{\pi_0}$ 作用 $\Phi^*$ 得到 $\Phi^*(v^{\pi_0})$, 但是 $\Phi^*(v^{\pi_0})$ 可能不是任何一个 policy 的 value function.

可以从两个层面入手: 首先, 考虑 $\Phi^*$ 的唯一不动点 $\mathbf v^*$, 它必然是某个 optimal policy $\pi^*$ 的 value function, 因为只要取 $\pi^*(s) = \arg\max\limits_{a \in \mathcal A}\left[R(s, a) + \gamma\sum\limits_{s' \in \mathcal S}P_{s, s'}^a\mathbf v^*(s')\right]$ 即可.

其次, 对于任意的 $\mathbf v \in \mathbb R^N$, 我们都可以定义关于 $\mathbf v$ 的 greedy policy $\pi$, $\pi(s) = \arg\max\limits_{a \in \mathcal A}\left[R(s, a) + \gamma\sum\limits_{s' \in \mathcal S}P_{s, s'}^a\mathbf v(s')\right]$.

现在, 我们从初始 policy $\pi_0$ 出发, 记 $\mathbf v_0 = v^{\pi_0}$ 为 $\pi_0$ 的 value function, 迭代计算 $\mathbf v_{k+1} = \Phi^*(\mathbf v_k)$, 再记 $\pi_k$ 为 $\mathbf v_k$ 诱导的 greedy policy, $v^{\pi_k}$ 为 $\pi_k$ 的 value function.

\begin{theorem}
	$\|v^{\pi_k} - \mathbf v^*\|_{\infty} \le \frac{\gamma}{1 - \gamma}\|\mathbf v_k - \mathbf v^*\|_{\infty}$.
\end{theorem}

\end{document}

